# -*- coding: utf-8 -*-
"""Copy of DL_Final_2023.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fRZ0SPnwOuuHT8QsgR4nTVqLV2Sf5YJj

# Final Project - Deep Learning 
Hello dear students,<br> this is the template notebook. Please click on the "File" tab and then on "Save a copy into drive".

---
<br>

### Name and ID:
Student 1: Yuval Uner, 322558842
<br>
Student 2: Omri Ben Hemo, 313255242

### Goodluck!

### Download the explainer.md file
"""

!gdown --id 1OCPWq2tUmqtMyw6RuvMTrRBttnczgMKc

"""# Load dataset from Kaggle"""

! pip install -q kaggle
!gdown --id 1vATx2e-MujESmBtvDtjsipJC-V8LaQ2g
!mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

! kaggle competitions download -c gan-getting-started
! unzip gan-getting-started.zip

"""# Select our training set

## Installs and permissions
"""

!pip install tensorflow_addons

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
from tensorflow_addons import image
import tensorflow_addons as tfa
from PIL import Image
import os
import random

!chmod 777 -R monet_tfrec/
!chmod 777 -R photo_tfrec/

"""## Loading the dataset and choosing the 30 paintings

In our selection of images, we used an algorithm for finding the vibrancy of an image.<br/>
We then took 30 images, such that 10 of them are very vibrant, 10 are in the middle range, and 10 are not vibrant.

The first function, find_dominant_colors, finds the 3 most dominant colors in the image and returns them.

#### Image vibrancy detection algorithm
"""

def find_dominant_colors(filename):
    #Resizing parameters
    width, height = 150,150
    image = Image.open(filename)
    image = image.resize((width, height),resample = 0)
    #Get colors from image object
    pixels = image.getcolors(width * height)
    #Sort them by count number(first element of tuple)
    sorted_pixels = np.array(sorted(pixels, key=lambda t: t[0]))
    #Get the most frequent color
    dominant_color = sorted_pixels[-3:, 1]
    return dominant_color

"""The second function, compute_vibrancy (based on an answer from <a href=https://stackoverflow.com/a/61720456>this stackoverflow answer</a>), computes the vibrancy of a color, and returns the maximum value it found."""

def compute_vibrancy(filename):
  vibrancy = []
  dominant_colors = find_dominant_colors(filename)
  for color in dominant_colors:
    colorfulness = 0

    if color != (0, 0, 0):
      min = np.min(color)
      max = np.max(color)
      colorfulness = ((max+ min) * (max-min))/max
    
    vibrancy.append(colorfulness)
  return np.max(vibrancy)

"""Monet images are loaded such that their vibrancy is also calculated, while normal pictures do not need that part.

#### Image loading and resizing
"""

class MonetImage:

  def __init__(self, as_arr, vibrancy):
    self.as_arr = as_arr
    self.vibrancy = vibrancy

def load_monet_images(directory):
    images = []
    for filename in os.listdir(directory):
        if filename.endswith(".jpg"):
            # Open the image file
            image = Image.open(os.path.join(directory, filename))
            image = image.resize((320, 320))
            vibrancy = compute_vibrancy(os.path.join(directory, filename)) 
            # Convert the image to a numpy array
            image_array = np.asarray(image)
            images.append(MonetImage(image_array, vibrancy))
    return images

def load_images(directory):
    images = []
    for filename in os.listdir(directory):
        if filename.endswith(".jpg"):
            # Open the image file
            image = Image.open(os.path.join(directory, filename))
            image = image.resize((320, 320))
            # Convert the image to a numpy array
            image_array = np.asarray(image)
            images.append(image_array)
    return np.array(images)

monet_images = load_monet_images("monet_jpg/")
photo_images = load_images("photo_jpg/")

"""Finally, we sort the images by their vibrancy and choose 30 images, such that there will be a distribution of vibrancy and colors over the dataset.</br>
This is to try and make sure that the model has a varied set of examples and does not learn to produce only images of a certain type.

#### Image selection
"""

monet_images.sort(key=lambda x: x.vibrancy)
monet_dataset = np.array([monet_images[i].as_arr for i in range(len(monet_images)) if i % 10 == 0])
np.random.shuffle(monet_images)

print(monet_dataset.shape)

"""## Display images from the sets"""

plt.subplot(121)
plt.title('Photo')
plt.imshow(random.choice(photo_images))

plt.subplot(122)
plt.title('Monet')
plt.imshow(random.choice(monet_dataset))

"""# Cycle GAN"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_addons as tfa

import matplotlib.pyplot as plt
import numpy as np

try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print('Device:', tpu.master())
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
except:
    strategy = tf.distribute.get_strategy()

AUTOTUNE = tf.data.experimental.AUTOTUNE

"""## Build generator with UNET architecture

We'll be using a UNET architecture for our CycleGAN. To build our generator, let's first define our downsample and upsample methods.

### Downsample
The downsample reduces the 2D dimensions, the width and height, of the image by the stride.
Since the stride is 2, the filter is applied to every other pixel, hence reducing the weight and height by 2.
"""

OUTPUT_CHANNELS = 3

def downsample(filters, size, apply_instancenorm=True):
    initializer = tf.random_normal_initializer(0., 0.02)
    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)

    result = keras.Sequential()
    result.add(layers.Conv2D(filters, size, strides=2, padding='same',
                             kernel_initializer=initializer, use_bias=False))

    if apply_instancenorm:
        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))

    result.add(layers.LeakyReLU())

    return result

"""### Upsample
Upsample does the opposite of downsample and increases the dimensions of the of the image. Conv2DTranspose does basically the opposite of a Conv2D layer.
"""

def upsample(filters, size, apply_dropout=False):
    initializer = tf.random_normal_initializer(0., 0.02)
    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)

    result = keras.Sequential()
    result.add(layers.Conv2DTranspose(filters, size, strides=2,
                                      padding='same',
                                      kernel_initializer=initializer,
                                      use_bias=False))

    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))

    if apply_dropout:
        result.add(layers.Dropout(0.5))

    result.add(layers.ReLU())

    return result

"""### Generator
The generator first downsamples the input image and then upsample while establishing long skip connections.
"""

def Generator():
    inputs = layers.Input(shape=[320,320,3])

    # bs = batch size
    down_stack = [
        downsample(64, 4, apply_instancenorm=False), # (bs, 160, 160, 64)
        downsample(128, 4), # (bs, 80, 80, 128)
        downsample(256, 4), # (bs, 40, 40, 256)
        downsample(512, 4), # (bs, 20, 20, 512)
        downsample(512, 4), # (bs, 10, 10, 512)
        downsample(512, 4), # (bs, 5, 5, 512)
    ]

    up_stack = [
        upsample(512, 4, apply_dropout=True), # (bs, 10, 10, 1024)
        upsample(512, 4), # (bs, 20, 20, 1024)
        upsample(256, 4), # (bs, 40, 40, 512)
        upsample(128, 4), # (bs, 80, 80, 256)
        upsample(64, 4), # (bs, 160, 160, 128)
    ]

    initializer = tf.random_normal_initializer(0., 0.02)
    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,
                                  strides=2,
                                  padding='same',
                                  kernel_initializer=initializer,
                                  activation='tanh') # (bs, 320, 320, 3)

    x = inputs

    # Downsampling through the model
    skips = []
    for down in down_stack:
        x = down(x)
        skips.append(x)

    skips = reversed(skips[:-1])

    # Upsampling and establishing the skip connections
    for up, skip in zip(up_stack, skips):
        x = up(x)
        x = layers.Concatenate()([x, skip])

    x = last(x)

    return keras.Model(inputs=inputs, outputs=x)

"""## Build the discriminator
The discriminator takes in the input image and classifies it as real or fake (generated). Instead of outputing a single node, the discriminator outputs a smaller 2D image with higher pixel values indicating a real classification and lower values indicating a fake classification.

### Define discriminator
"""

def Discriminator():
    initializer = tf.random_normal_initializer(0., 0.02)
    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)

    inp = layers.Input(shape=[320, 320, 3], name='input_image')

    x = inp

    down1 = downsample(64, 4, False)(x) # (bs, 160, 160, 64)
    down2 = downsample(128, 4)(down1) # (bs, 80, 80, 128)
    down3 = downsample(256, 4)(down2) # (bs, 40, 40, 256)

    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 40, 40, 256)
    conv = layers.Conv2D(512, 4, strides=1,
                         kernel_initializer=initializer,
                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)

    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)

    leaky_relu = layers.LeakyReLU()(norm1)

    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)

    last = layers.Conv2D(1, 4, strides=1,
                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)

    return tf.keras.Model(inputs=inp, outputs=last)

"""### Build 2 generators and 2 disciminators for the cycleGAN"""

with strategy.scope():
    monet_generator = Generator() # transforms photos to Monet-esque paintings
    photo_generator = Generator() # transforms Monet paintings to be more like photos

    monet_discriminator = Discriminator() # differentiates real Monet paintings and generated Monet paintings
    photo_discriminator = Discriminator() # differentiates real photos and generated photos

print("monet generator model\n")
monet_generator.summary()
print("\n\nphoto generator model\n")
photo_generator.summary()
print("\n\nmonet discriminator model\n")
monet_discriminator.summary()
print("\n\nphoto discriminator generator model\n")
photo_discriminator.summary()

"""## Prepares the data for train"""

def cast_to_float(image):
    image = tf.cast(image, tf.float32)
    image = (image - 127.5) / 127.5
    return image

# Define the batch size and the number of training iterations
batch_size = 1

# Create a dataset from the monet_dataset variable
monet_data = tf.data.Dataset.from_tensor_slices(monet_dataset)

# Apply the cast function to the dataset
monet_data = monet_data.map(cast_to_float)

# Batch the dataset
monet_data = monet_data.batch(batch_size)

# Define the batch size and the number of training iterations
batch_size = 1

# Create a dataset from the monet_dataset variable
photo_data = tf.data.Dataset.from_tensor_slices(photo_images)

# Apply the cast function to the dataset
photo_data = photo_data.map(cast_to_float)

# Batch the dataset
photo_data = photo_data.batch(batch_size)

# Create an iterator for the dataset
iter_photo = iter(tf.data.Dataset.from_tensor_slices(photo_images).batch(1))

# Get the next batch of images
iter_photo = iter_photo.get_next()

# Normalize the images
iter_photo = tf.cast(iter_photo, tf.float32)
iter_photo = (iter_photo - 127.5) / 127.5

"""### Monet generator
Test the monet generator before the train.
"""

example_photo = iter_photo
to_monet = monet_generator(example_photo)

plt.subplot(1, 2, 1)
plt.title("Original Photo")
plt.imshow(example_photo[0] * 0.5 + 0.5)

plt.subplot(1, 2, 2)
plt.title("Monet-esque Photo")
plt.imshow(to_monet[0] * 0.5 + 0.5)
plt.show()

"""## Build the CycleGAN model
In the cycleGAN model during the training step, the model transforms a photo to a Monet painting and then back to a photo. The difference between the original photo and the twice-transformed photo is the cycle-consistency loss. We want the original photo and the twice-transformed photo to be similar to one another.

### CycleGAN class
"""

class CycleGan(keras.Model):
    def __init__(
        self, monet_generator, photo_generator, monet_discriminator, photo_discriminator, lambda_cycle=10, ):
        super(CycleGan, self).__init__()
        self.m_gen = monet_generator
        self.p_gen = photo_generator
        self.m_disc = monet_discriminator
        self.p_disc = photo_discriminator
        self.lambda_cycle = lambda_cycle
        
    def compile(self, m_gen_optimizer, p_gen_optimizer, m_disc_optimizer, p_disc_optimizer, gen_loss_fn, disc_loss_fn, cycle_loss_fn, identity_loss_fn ):
        super(CycleGan, self).compile()
        self.m_gen_optimizer = m_gen_optimizer
        self.p_gen_optimizer = p_gen_optimizer
        self.m_disc_optimizer = m_disc_optimizer
        self.p_disc_optimizer = p_disc_optimizer
        self.gen_loss_fn = gen_loss_fn
        self.disc_loss_fn = disc_loss_fn
        self.cycle_loss_fn = cycle_loss_fn
        self.identity_loss_fn = identity_loss_fn
        
    def call(self, inputs):
        real_monet, real_photo = inputs
        
        # photo to monet back to photo
        fake_monet = self.m_gen(real_photo, training=True)
        cycled_photo = self.p_gen(fake_monet, training=True)

        # monet to photo back to monet
        fake_photo = self.p_gen(real_monet, training=True)
        cycled_monet = self.m_gen(fake_photo, training=True)

        # generating itself
        same_monet = self.m_gen(real_monet, training=True)
        same_photo = self.p_gen(real_photo, training=True)

        return fake_monet, cycled_photo, fake_photo, cycled_monet, same_monet, same_photo

    def train_step(self, batch_data):
        real_monet, real_photo = batch_data
        
        with tf.GradientTape(persistent=True) as tape:
            # get outputs from the model call
            fake_monet, cycled_photo, fake_photo, cycled_monet, same_monet, same_photo = self([real_monet, real_photo])

            # discriminator used to check, inputing real images
            disc_real_monet = self.m_disc(real_monet, training=True)
            disc_real_photo = self.p_disc(real_photo, training=True)

            # discriminator used to check, inputing fake images
            disc_fake_monet = self.m_disc(fake_monet, training=True)
            disc_fake_photo = self.p_disc(fake_photo, training=True)

            # evaluates generator loss
            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)
            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)

            # evaluates total cycle consistency loss
            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)

            # evaluates total generator loss
            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)
            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)

            # evaluates discriminator loss
            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)
            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)

        # Calculate the gradients for generator and discriminator
        monet_generator_gradients = tape.gradient(total_monet_gen_loss,
                                                  self.m_gen.trainable_variables)
        photo_generator_gradients = tape.gradient(total_photo_gen_loss,
                                                  self.p_gen.trainable_variables)

        monet_discriminator_gradients = tape.gradient(monet_disc_loss,
                                                      self.m_disc.trainable_variables)
        photo_discriminator_gradients = tape.gradient(photo_disc_loss,
                                                      self.p_disc.trainable_variables)

        # Apply the gradients to the optimizer
        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,
                                                 self.m_gen.trainable_variables))

        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,
                                                 self.p_gen.trainable_variables))

        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,
                                                  self.m_disc.trainable_variables))

        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,
                                                  self.p_disc.trainable_variables))
        
        return {
            "monet_gen_loss": total_monet_gen_loss,
            "photo_gen_loss": total_photo_gen_loss,
            "monet_disc_loss": monet_disc_loss,
            "photo_disc_loss": photo_disc_loss
        }

"""### Define the loss functions"""

with strategy.scope():
    def discriminator_loss(real, generated):
        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)

        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)

        total_disc_loss = real_loss + generated_loss

        return total_disc_loss * 0.5

with strategy.scope():
    def generator_loss(generated):
        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)

with strategy.scope():
    def calc_cycle_loss(real_image, cycled_image, LAMBDA):
        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))

        return LAMBDA * loss1

with strategy.scope():
    def identity_loss(real_image, same_image, LAMBDA):
        loss = tf.reduce_mean(tf.abs(real_image - same_image))
        return LAMBDA * 0.5 * loss

"""### Choose optimizers"""

with strategy.scope():
    monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

    monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

"""### compile the model"""

with strategy.scope():
    cycle_gan_model = CycleGan(
        monet_generator, photo_generator, monet_discriminator, photo_discriminator
    )

    cycle_gan_model.compile(
        m_gen_optimizer = monet_generator_optimizer,
        p_gen_optimizer = photo_generator_optimizer,
        m_disc_optimizer = monet_discriminator_optimizer,
        p_disc_optimizer = photo_discriminator_optimizer,
        gen_loss_fn = generator_loss,
        disc_loss_fn = discriminator_loss,
        cycle_loss_fn = calc_cycle_loss,
        identity_loss_fn = identity_loss
    )

def plot_losses_cycleGAN(monet_gen_loss, photo_gen_loss, monet_disc_loss, photo_disc_loss):
    plt.plot(monet_gen_loss, label="monet_gen_loss")
    plt.xlabel("Iterations")
    plt.ylabel("Loss")
    plt.legend()
    plt.title("Training Losses - Generator")
    plt.show()

    plt.plot(photo_gen_loss, label="photo_gen_loss")
    plt.xlabel("Iterations")
    plt.ylabel("Loss")
    plt.title("Training Losses - Generator")
    plt.legend()
    plt.show()

    plt.plot(monet_disc_loss, label="monet_disc_loss")
    plt.xlabel("Iterations")
    plt.ylabel("Loss")
    plt.title("Training Losses - Generator")
    plt.legend()
    plt.show()

    plt.plot(photo_disc_loss, label="photo_disc_loss")
    plt.xlabel("Iterations")
    plt.ylabel("Loss")
    plt.title("Training Losses - Generator")
    plt.legend()
    plt.show()

"""### Fit the model"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# history = cycle_gan_model.fit(
#     tf.data.Dataset.zip((monet_data, photo_data)),
#     epochs=50
# )

"""### Results"""

def get_mean_epoch_losses(losses_list):
    # calculate mean loss for each iteration of each epoch
    epoch_losses = np.mean(losses_list, axis=(2, 3))

    # calculate mean loss for each epoch
    mean_epoch_losses = np.mean(epoch_losses, axis=1)

    # reshape to (50, 1) to match desired output shape
    mean_epoch_losses = np.reshape(mean_epoch_losses, (np.shape(losses_list)[0], 1))

    return mean_epoch_losses

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# 
# monet_gen_loss = get_mean_epoch_losses(history.history['monet_gen_loss'])
# photo_gen_loss = get_mean_epoch_losses(history.history['photo_gen_loss'])
# monet_disc_loss = get_mean_epoch_losses(history.history['monet_disc_loss'])
# photo_disc_loss = get_mean_epoch_losses(history.history['photo_disc_loss'])

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# plot_losses_cycleGAN(monet_gen_loss, photo_gen_loss, monet_disc_loss, photo_disc_loss)

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# _, ax = plt.subplots(5, 2, figsize=(12, 12))
# for i, img in enumerate(photo_data.shuffle(buffer_size = 20).take(5)):
#     prediction = monet_generator(img, training=False)[0].numpy()
#     prediction = (prediction * 127.5 + 127.5).astype(np.uint8)
#     img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)
# 
#     ax[i, 0].imshow(img)
#     ax[i, 1].imshow(prediction)
#     ax[i, 0].set_title("Input Photo")
#     ax[i, 1].set_title("Monet-esque")
#     ax[i, 0].axis("off")
#     ax[i, 1].axis("off")
# plt.show()

"""## First attempt to improve - Optimizers

### different optimizers

Initially, we'll attempt to modify the optimizers. For the first trial, we'll switch to using SGD with momentum instead of ADAM.

Once we determine the optimal learning rate and momentum values for SGD, we will proceed to train it using these settings and assess the outcome.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# for lr in [0.0001, 0.00005, 0.00001]:
#   for mn in [0.99, 0.95, 0.9, 0.8]:
#     print(f'The learning rate is: {lr}, the momentum is: {mn}')
#     with strategy.scope():
#         monet_generator_optimizer = tf.keras.optimizers.SGD(learning_rate = lr, momentum = mn)
#         photo_generator_optimizer = tf.keras.optimizers.SGD(learning_rate = lr, momentum = mn)
# 
#         monet_discriminator_optimizer = tf.keras.optimizers.SGD(learning_rate = lr, momentum = mn)
#         photo_discriminator_optimizer = tf.keras.optimizers.SGD(learning_rate = lr, momentum = mn)
# 
#         monet_generator = Generator() # transforms photos to Monet-esque paintings
#         photo_generator = Generator() # transforms Monet paintings to be more like photos
# 
#         monet_discriminator = Discriminator() # differentiates real Monet paintings and generated Monet paintings
#         photo_discriminator = Discriminator() # differentiates real photos and generated photos
# 
#         cycle_gan_model = CycleGan(monet_generator, photo_generator, monet_discriminator, photo_discriminator)
# 
#         cycle_gan_model.compile(
#             m_gen_optimizer = monet_generator_optimizer,
#             p_gen_optimizer = photo_generator_optimizer,
#             m_disc_optimizer = monet_discriminator_optimizer,
#             p_disc_optimizer = photo_discriminator_optimizer,
#             gen_loss_fn = generator_loss,
#             disc_loss_fn = discriminator_loss,
#             cycle_loss_fn = calc_cycle_loss,
#             identity_loss_fn = identity_loss
#         )
#         cycle_gan_model.fit(tf.data.Dataset.zip((monet_data, photo_data)),epochs=5)

"""After determining the optimal learning rate and momentum for the SGD optimizer, we will proceed to train the model and compare its performance to that of the ADAM optimizer."""

lr = 0.00001
mn = 0.9
with strategy.scope():
        monet_generator_optimizer = tf.keras.optimizers.SGD(learning_rate = lr, momentum = mn)
        photo_generator_optimizer = tf.keras.optimizers.SGD(learning_rate = lr, momentum = mn)

        monet_discriminator_optimizer = tf.keras.optimizers.SGD(learning_rate = lr, momentum = mn)
        photo_discriminator_optimizer = tf.keras.optimizers.SGD(learning_rate = lr, momentum = mn)

"""### Architecture"""

with strategy.scope():
    monet_generator = Generator() # transforms photos to Monet-esque paintings
    photo_generator = Generator() # transforms Monet paintings to be more like photos

    monet_discriminator = Discriminator() # differentiates real Monet paintings and generated Monet paintings
    photo_discriminator = Discriminator() # differentiates real photos and generated photos

with strategy.scope():
    cycle_gan_model = CycleGan(monet_generator, photo_generator, monet_discriminator, photo_discriminator)

    cycle_gan_model.compile(
        m_gen_optimizer = monet_generator_optimizer,
        p_gen_optimizer = photo_generator_optimizer,
        m_disc_optimizer = monet_discriminator_optimizer,
        p_disc_optimizer = photo_discriminator_optimizer,
        gen_loss_fn = generator_loss,
        disc_loss_fn = discriminator_loss,
        cycle_loss_fn = calc_cycle_loss,
        identity_loss_fn = identity_loss
    )

"""### Fit and results"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# cycle_gan_model.fit(
#     tf.data.Dataset.zip((monet_data, photo_data)),
#     epochs=20
# )

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# _, ax = plt.subplots(5, 2, figsize=(12, 12))
# for i, img in enumerate(photo_data.shuffle(buffer_size = 30).take(5)):
#     prediction = monet_generator(img, training=False)[0].numpy()
#     prediction = (prediction * 127.5 + 127.5).astype(np.uint8)
#     img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)
# 
#     ax[i, 0].imshow(img)
#     ax[i, 1].imshow(prediction)
#     ax[i, 0].set_title("Input Photo")
#     ax[i, 1].set_title("Monet-esque")
#     ax[i, 0].axis("off")
#     ax[i, 1].axis("off")
# plt.show()

"""Despite attempting to switch to the SGD optimizer and selecting the optimal parameters for it, the performance of the model remained unsatisfactory. As a result, we will revert back to using the ADAM optimizer.

## Second attempt to improve - change batch number
In this attempt, first i will try to improve the number of batch.

### Change batch number

In the first version of the model, I experimented with a batch size of 1. While this provided a good starting point for my analysis, I now want to explore how changing the batch size can impact the model's performance. Therefore, I plan to modify the batch size to 30 and re-run the analysis.
"""

# Define the batch size and the number of training iterations
batch_size = 5

# Create a dataset from the monet_dataset variable
monet_data = tf.data.Dataset.from_tensor_slices(monet_dataset)

# Apply the cast function to the dataset
monet_data = monet_data.map(cast_to_float)

# Batch the dataset
monet_data = monet_data.batch(batch_size)

# Define the batch size and the number of training iterations
batch_size = 5

# Create a dataset from the photo_images variable
photo_data = tf.data.Dataset.from_tensor_slices(photo_images)

# Apply the cast function to the dataset
photo_data = photo_data.map(cast_to_float)

# Batch the dataset
photo_data = photo_data.batch(batch_size)

"""### Fit and results"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# cycle_gan_model.fit(
#     tf.data.Dataset.zip((monet_data, photo_data)),
#     epochs=20
# )

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# _, ax = plt.subplots(5, 2, figsize=(12, 12))
# for i, img in enumerate(photo_data.shuffle(buffer_size = 30).take(5)):
#     prediction = monet_generator(img, training=False)[0].numpy()
#     prediction = (prediction * 127.5 + 127.5).astype(np.uint8)
#     img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)
# 
#     ax[i, 0].imshow(img)
#     ax[i, 1].imshow(prediction)
#     ax[i, 0].set_title("Input Photo")
#     ax[i, 1].set_title("Monet-esque")
#     ax[i, 0].axis("off")
#     ax[i, 1].axis("off")
# plt.show()

"""### Change batch number again
Our first attempt was to change the batch size to 5. However, we quickly noticed that the loss stayed almost the same as before, which was a bit disappointing. We suspected that this could be due to the fact that our dataset was not large enough to benefit from smaller batch sizes.

Next, we tried increasing the batch size to 10, hoping that this would lead to better results. However, our computer's 15 GB RAM was not sufficient to handle this size, and we encountered memory errors during training. This was a setback, as we had hoped to test the impact of larger batch sizes on our model's performance.

To work around this issue, we decided to try a compromise and set the batch size to 2. This allowed us to continue our experiments without running into memory issues. We are currently training our model with this new batch size and will monitor the loss to see if we observe any improvements.
"""

# Define the batch size and the number of training iterations
batch_size = 2

# Create a dataset from the monet_dataset variable
monet_data = tf.data.Dataset.from_tensor_slices(monet_dataset)

# Apply the cast function to the dataset
monet_data = monet_data.map(cast_to_float)

# Batch the dataset
monet_data = monet_data.batch(batch_size)

# Define the batch size and the number of training iterations
batch_size = 2

# Create a dataset from the photo_images variable
photo_data = tf.data.Dataset.from_tensor_slices(photo_images)

# Apply the cast function to the dataset
photo_data = photo_data.map(cast_to_float)

# Batch the dataset
photo_data = photo_data.batch(batch_size)

"""### Reset the weights"""

with strategy.scope():
    monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

    monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

with strategy.scope():
    cycle_gan_model = CycleGan(
        monet_generator, photo_generator, monet_discriminator, photo_discriminator
    )

    cycle_gan_model.compile(
        m_gen_optimizer = monet_generator_optimizer,
        p_gen_optimizer = photo_generator_optimizer,
        m_disc_optimizer = monet_discriminator_optimizer,
        p_disc_optimizer = photo_discriminator_optimizer,
        gen_loss_fn = generator_loss,
        disc_loss_fn = discriminator_loss,
        cycle_loss_fn = calc_cycle_loss,
        identity_loss_fn = identity_loss
    )

"""### Fit again and results"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# cycle_gan_model.fit(
#     tf.data.Dataset.zip((monet_data, photo_data)),
#     epochs=20
# )

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# _, ax = plt.subplots(5, 2, figsize=(12, 12))
# for i, img in enumerate(photo_data.shuffle(buffer_size = 30).take(5)):
#     prediction = monet_generator(img, training=False)[0].numpy()
#     prediction = (prediction * 127.5 + 127.5).astype(np.uint8)
#     img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)
# 
#     ax[i, 0].imshow(img)
#     ax[i, 1].imshow(prediction)
#     ax[i, 0].set_title("Input Photo")
#     ax[i, 1].set_title("Monet-esque")
#     ax[i, 0].axis("off")
#     ax[i, 1].axis("off")
# plt.show()

"""## Third attempt to improve - change learning rate
In this attempt, we will experiment with different values for the ADAM optimizer's parameters and observe their effects on the model's accuracy and convergence. By doing so, we hope to optimize the training process and achieve better results for our task.

### Preparing the data again
Let's reset the batch number to 1 since we have observed that it is the most effective way to train the model in terms of both speed and loss.
"""

# Define the batch size and the number of training iterations
batch_size = 1

# Create a dataset from the monet_dataset variable
monet_data = tf.data.Dataset.from_tensor_slices(monet_dataset)

# Apply the cast function to the dataset
monet_data = monet_data.map(cast_to_float)

# Batch the dataset
monet_data = monet_data.batch(batch_size)

# Define the batch size and the number of training iterations
batch_size = 1

# Create a dataset from the photo_images variable
photo_data = tf.data.Dataset.from_tensor_slices(photo_images)

# Apply the cast function to the dataset
photo_data = photo_data.map(cast_to_float)

# Batch the dataset
photo_data = photo_data.batch(batch_size)

"""### Architecture

Reset the weights
"""

with strategy.scope():
    monet_generator = Generator() # transforms photos to Monet-esque paintings
    photo_generator = Generator() # transforms Monet paintings to be more like photos

    monet_discriminator = Discriminator() # differentiates real Monet paintings and generated Monet paintings
    photo_discriminator = Discriminator() # differentiates real photos and generated photos

"""Review a few common parameters and select the optimal one based on both speed and loss."""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# for lr in [0.0002, 0.00005, 0.00001]:
#   for beta in [0.2, 0.5, 0.6]:
#     with strategy.scope():
#         print(f'\n\nThe learning rate is: {lr}, and the beta_1 is: {beta}.')
#         monet_generator_optimizer = tf.keras.optimizers.Adam(learning_rate = lr, beta_1 = beta)
#         photo_generator_optimizer = tf.keras.optimizers.Adam(learning_rate = lr, beta_1 = beta)
# 
#         monet_discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate = lr, beta_1 = beta)
#         photo_discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate = lr, beta_1 = beta)
# 
#         monet_generator = Generator() # transforms photos to Monet-esque paintings
#         photo_generator = Generator() # transforms Monet paintings to be more like photos
# 
#         monet_discriminator = Discriminator() # differentiates real Monet paintings and generated Monet paintings
#         photo_discriminator = Discriminator() # differentiates real photos and generated photos
# 
#         cycle_gan_model = CycleGan(monet_generator, photo_generator, monet_discriminator, photo_discriminator)
# 
#         cycle_gan_model.compile(
#             m_gen_optimizer = monet_generator_optimizer,
#             p_gen_optimizer = photo_generator_optimizer,
#             m_disc_optimizer = monet_discriminator_optimizer,
#             p_disc_optimizer = photo_discriminator_optimizer,
#             gen_loss_fn = generator_loss,
#             disc_loss_fn = discriminator_loss,
#             cycle_loss_fn = calc_cycle_loss,
#             identity_loss_fn = identity_loss
#         )
# 
#         cycle_gan_model.fit(tf.data.Dataset.zip((monet_data, photo_data)),epochs=5)

"""###Conclusions
Based on the results of the experiment, it is evident that the optimal values for the learning rate and beta parameter are 0.0002 and 0.6, respectively. These values produced the highest quality training outcome for the model.

## Final CycleGAN model 

Following a series of tests to determine the optimal parameters for the model, we have arrived at the final model.

### Architecture
"""

# %%script echo skipping
with strategy.scope():
    monet_generator = Generator() # transforms photos to Monet-esque paintings
    photo_generator = Generator() # transforms Monet paintings to be more like photos

    monet_discriminator = Discriminator() # differentiates real Monet paintings and generated Monet paintings
    photo_discriminator = Discriminator() # differentiates real photos and generated photos

# %%script echo skipping
with strategy.scope():
    monet_generator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.6)
    photo_generator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.6)

    monet_discriminator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.6)
    photo_discriminator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.6)

# %%script echo skipping
with strategy.scope():
    cycle_gan_model = CycleGan(
        monet_generator, photo_generator, monet_discriminator, photo_discriminator
    )

    cycle_gan_model.compile(
        m_gen_optimizer = monet_generator_optimizer,
        p_gen_optimizer = photo_generator_optimizer,
        m_disc_optimizer = monet_discriminator_optimizer,
        p_disc_optimizer = photo_discriminator_optimizer,
        gen_loss_fn = generator_loss,
        disc_loss_fn = discriminator_loss,
        cycle_loss_fn = calc_cycle_loss,
        identity_loss_fn = identity_loss
    )

"""### Fit and result"""

# %%script echo skipping
history = cycle_gan_model.fit(
    tf.data.Dataset.zip((monet_data, photo_data)),
    epochs=50
)

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# from google.colab import drive
# drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# cycle_gan_model.save("/content/drive/MyDrive/Deep Learning/Final/best_cycleGAN_model")

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# monet_generator.save("/content/drive/MyDrive/Deep Learning/Final/best_cycleGAN_monet_generator")

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# monet_gen_loss = get_mean_epoch_losses(history.history['monet_gen_loss'])
# photo_gen_loss = get_mean_epoch_losses(history.history['photo_gen_loss'])
# monet_disc_loss = get_mean_epoch_losses(history.history['monet_disc_loss'])
# photo_disc_loss = get_mean_epoch_losses(history.history['photo_disc_loss'])

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# plot_losses_cycleGAN(monet_gen_loss, photo_gen_loss, monet_disc_loss, photo_disc_loss)

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# _, ax = plt.subplots(5, 2, figsize=(12, 12))
# for i, img in enumerate(photo_data.shuffle(buffer_size = 30).take(5)):
#     prediction = monet_generator(img, training=False)[0].numpy()
#     prediction = (prediction * 127.5 + 127.5).astype(np.uint8)
#     img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)
# 
#     ax[i, 0].imshow(img)
#     ax[i, 1].imshow(prediction)
#     ax[i, 0].set_title("Input Photo")
#     ax[i, 1].set_title("Monet-esque")
#     ax[i, 0].axis("off")
#     ax[i, 1].axis("off")
# plt.show()

"""# Simple GAN

## First model - our base model
"""

from keras.layers import Input, Dense, Reshape, LeakyReLU, Dropout, Flatten
from keras.layers import BatchNormalization, Activation, ZeroPadding2D
from keras.layers.convolutional import UpSampling2D, Conv2D, MaxPooling2D, Conv2DTranspose
from keras.models import Sequential, Model
from keras import layers
from tensorflow import keras

# Define the shape of the input images and the model's output
img_shape = (256, 256, 3)
generator_output_shape = (320, 320, 3)

"""### Model architecture

Our first model, the base model, has the simplest architecture of all the models.<br/>
Its generator has a few convolution layers, and some dense layers, such that the number of parameters was one that avoided crashing the GPU of the colab notebook (yes, the session crashed many times before settling on this architecture).</br>
</br>
The discriminator is also very simple - a simple convolutional network without even a pooling layer, and a few filters.</br>
</br>
Also, in order to output 320x320 pictures as requested, the model expects to accept an image that was already resized to 320x320 instead of 256x256 images.
"""

def build_generator():
    model = Sequential()
    model.add(Input(shape=generator_output_shape))
    model.add(Flatten())
    model.add(Dense(256))
    model.add(layers.LeakyReLU())
    model.add(Dense(516))
    model.add(layers.LeakyReLU())
    model.add(Dense(256))
    model.add(layers.LeakyReLU())
    model.add(Dense(np.prod(generator_output_shape)))
    model.add(layers.LeakyReLU())
    model.add(Reshape(generator_output_shape))
    model.add(layers.Conv2D(32, (5, 5), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(32, (5, 5), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(3, (5, 5), padding='same', activation='tanh'))
    return model

# Define the discriminator network
def build_discriminator():
    model = Sequential()
    model.add(layers.Conv2D(32, (5, 5), padding='same', input_shape=generator_output_shape))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(32, (5, 5), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Flatten())
    model.add(layers.Dense(1, activation='sigmoid'))
    return model

"""### Hyper-parameters

We had a simple method for deciding on hyperparameters for our base model - copy paste the model architecture into Pycharm, then let github copilot decide on the optimizer and hyperparameters, as it is likely to choose what others commonly choose.<br/>
Naturally, these will be changed later when experimenting.
"""

gan = Sequential()
discriminator = build_discriminator()
discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.RMSprop(lr=0.0008, clipvalue=1.0, decay=1e-8))
discriminator.trainable = False
discriminator.summary()
generator = build_generator()
generator.summary()
gan.add(generator)
gan.add(discriminator)
gan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8))

"""### Preparing the data"""

# Define the batch size and the number of training iterations
batch_size = 32

# Create a dataset from the monet_dataset variable
monet_dataset = tf.data.Dataset.from_tensor_slices(monet_dataset)

# Repeat the dataset indefinitely
monet_dataset = monet_dataset.repeat()

# Batch the dataset
monet_dataset = monet_dataset.batch(batch_size)

# Create an iterator for the dataset
monet_iterator = iter(monet_dataset)

# Get the next batch of images
real_images = monet_iterator.get_next()

# Normalize the images
real_images = tf.cast(real_images, tf.float32)
real_images = (real_images - 127.5) / 127.5

def generate_and_show():
  noise = np.random.uniform(-1, 1, (batch_size, 320, 320, 3))
  generated_image = generator.predict(noise)

  # Convert the image back to the original scale
  generated_image = (generated_image + 1) * 127.5

  # Convert the image to uint8
  generated_image = np.uint8(generated_image)

  # Create a grid of 8x4 subplots
  fig, axes = plt.subplots(8, 4)

  fig.set_size_inches(18, 32)

  # Flatten the axes array so we can iterate through it easily
  axes = axes.ravel()

  # Iterate through the list of images and display them in the subplots
  for i, img in enumerate(generated_image):
      axes[i].imshow(img)

  # Show the plot
  plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# # Before training
# generate_and_show()

"""### Training and results

"""

def train(num_iter):
    d_losses = []
    g_losses = []
    for i in range(num_iter):
        # Train discriminator
        # Generate fake images
        noise = np.random.uniform(-1, 1, (batch_size, 320, 320, 3))
        fake_images = generator.predict(noise)
        # Get real images
        # Combine real and fake images
        x = np.concatenate((real_images, fake_images))
        # Create labels
        y = np.concatenate((np.random.uniform(0.9, 1,(batch_size, 1)), np.random.uniform(0, 0.1,(batch_size, 1))))
        # Train discriminator
        d_loss = discriminator.train_on_batch(x, y)
        d_losses.append(d_loss)
        # Train generator
        noise = np.random.uniform(-1, 1, (batch_size, 320, 320, 3))
        y = np.ones((batch_size, 1))
        g_loss = gan.train_on_batch(noise, y)
        g_losses.append(g_loss)
        print('Iteration: {}, Discriminator loss: {}, Generator loss: {}'.format(i, d_loss, g_loss))
    return d_losses, g_losses

"""We've chosen to go with 1000 epochs for the training of our base model.<br/>
This is due to GANs being notoriously hard to train.<br/>
Ofcourse, this too is subject to change later.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# d_losses, g_losses = train(1000)

def plot_losses(d_losses, g_losses):
    plt.plot(d_losses, label="Discriminator loss")
    plt.xlabel("Iterations")
    plt.ylabel("Loss")
    plt.legend()
    plt.title("Training Losses - Discriminator")
    plt.show()

    plt.plot(g_losses, label="Generator loss")
    plt.xlabel("Iterations")
    plt.ylabel("Loss")
    plt.title("Training Losses - Generator")
    plt.legend()
    plt.show()

"""As can be seen from the loss graphs, this model is all over the place.</br>
While it did show improvement over time, it did not particularly converge anywhere and was instead very noisy all throughout.

<b> We accidentally pressed the button to run the below cell again, after creating another session where the function was not defined.</br>
As such, we accidentally erased the graph from here, and it could not be produced again without re-training the model (which takes a long time).</br>
However, it can still be seen in the report.</b>
"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# plot_losses(d_losses, g_losses)

"""However, despite not converging, it <b>did</b> pick up on the style of the paintings, as can be seen by its output.<br/>
While the images do not seem like paintings of anything, they do have the same general style as Monet paintings.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# # After training
# generate_and_show()

"""## Improvement attempt 1 - better discriminator

The base architecture's discriminator is an extremely weak one - it does not utilize any max pooling layers, and only 2 convolution layers with few filters, making it very hard or even impossible for it to pick up on any high level features.</br>
As such, we added multiple new convolution layers to this architecture, as well as max pooling layers and more filters per layer for most layers.</br>
In addition, the epoch count was reduced to 300, as it seemed from the base model that around after the 200th iteration its noisy behavior was mostly the same, so we decided to give it 200 iterations + 50% safety margin.

### Architecture
"""

def build_generator():
    model = Sequential()
    model.add(Input(shape=generator_output_shape))
    model.add(Flatten())
    model.add(Dense(256))
    model.add(layers.LeakyReLU())
    model.add(Dense(516))
    model.add(layers.LeakyReLU())
    model.add(Dense(256))
    model.add(layers.LeakyReLU())
    model.add(Dense(np.prod(generator_output_shape)))
    model.add(layers.LeakyReLU())
    model.add(Reshape(generator_output_shape))
    model.add(layers.Conv2D(32, (5, 5), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(32, (5, 5), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(3, (5, 5), padding='same', activation='tanh'))
    return model

# Define the discriminator network
def build_discriminator():
    model = Sequential()
    model.add(layers.Conv2D(32, (5, 5), padding='same', input_shape=generator_output_shape))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(48, (5, 5), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(64, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(128, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(256, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(384, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.Flatten())
    model.add(layers.Dense(1, activation='sigmoid'))
    return model

gan = Sequential()
discriminator = build_discriminator()
discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.RMSprop(lr=0.0008, clipvalue=1.0, decay=1e-8))
discriminator.trainable = False
discriminator.summary()
generator = build_generator()
generator.summary()
gan.add(generator)
gan.add(discriminator)
gan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8))

"""### Training and results"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# d_losses, g_losses = train(300)

"""As can be seen from the graph, the loss values constantly went through ups and downs throughout the training process.</br>
However, they are more stable towards the end than they are at the beggining.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# plot_losses(d_losses, g_losses)

"""And the results are actually fairly good - the generated paintings look like Monet style paintings.</br>
However, there is an issue - the model is fixated on a certain type of painting, and generally produces only that with some variation.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# generate_and_show()

"""## Improvement attempt 2 - different generator and optimizer

### Architecture

With this model, we aimed to improve the architecture of the generator.</br>
We added batch normalization after every dense layer, Conv2DTranspose layers, more Conv2D layers, and batch normalization between those as well.</br>
The reason for why we constantly upsample and downsample the image will be explained in the training sub-section.
"""

def build_generator():
    model = Sequential()
    model.add(Input(shape=generator_output_shape))
    model.add(Flatten())
    model.add(Dense(256))
    model.add(layers.LeakyReLU())
    model.add(BatchNormalization(momentum=0.9))
    model.add(layers.LeakyReLU())
    model.add(BatchNormalization(momentum=0.9))
    model.add(Dense(np.prod(generator_output_shape)))
    model.add(layers.LeakyReLU())
    model.add(BatchNormalization(momentum=0.9))
    model.add(Reshape(generator_output_shape))
    model.add(layers.Conv2DTranspose(4, (5, 5), padding='same', strides=2))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(8, (5, 5), padding='same', strides=2))
    model.add(layers.LeakyReLU())
    model.add(BatchNormalization(momentum=0.9))
    model.add(layers.Conv2DTranspose(8, (5, 5), padding='same', strides=2))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(16, (5, 5), padding='same', strides=2))
    model.add(layers.LeakyReLU())
    model.add(BatchNormalization(momentum=0.9))
    model.add(layers.Conv2DTranspose(16, (5, 5), padding='same', strides=2))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(32, (5, 5), padding='same', strides=2))
    model.add(layers.LeakyReLU())
    model.add(BatchNormalization(momentum=0.9))
    model.add(layers.Conv2D(3, (5, 5), padding='same', activation='tanh'))
    return model

# Define the discriminator network
def build_discriminator():
    model = Sequential()
    model.add(layers.Conv2D(32, (5, 5), padding='same', input_shape=generator_output_shape))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(48, (5, 5), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(64, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(128, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(256, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(384, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.Flatten())
    model.add(layers.Dense(1, activation='sigmoid'))
    return model

"""### Optimizer
In addition to changing network's architecture, we have also changed its optimizer from RMSProp to Adam, as Adam optimizer should just generally be better for our purposes.
"""

gan = Sequential()
discriminator = build_discriminator()
discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate = 0.0008, beta_1=0.5))
discriminator.trainable = False
discriminator.summary()
generator = build_generator()
generator.summary()
gan.add(generator)
gan.add(discriminator)
gan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate = 0.0004, beta_1=0.5))

"""### Training

Due to the new architecture of the generator, we ran into a major problem - what happens when a GPU with 15 GBs of memory meets a [32, 1280, 1280, 32] tensor? The answer is, an exception.</br>
Due to the sheer size of the tensors, it became impossible to assign them to memory, making it impossible to generate batches any-more with the original architecture we intended to try and use.</br>
This is the reason for the constant usampling and downsampling seen in the architecture - any time we tried to train the model on anything but a stochastic process (which slowed down the process so much that we would run out of GPU time before a single model finished training) without downsampling the image again using a convolution layer, the training process would crash.</br>
Even with this, we still needed to make many adjustments - the transposed convolutions and convolution layers could only have very few filters each, or else it will crash.</br></br>
In this training process, when we generate the fake images for the discriminator, we generate them using 2 half batches.</br>
When we train the generator, we only give it 1 half batch, instead of a whole batch - this is, once again, because anything more would cause it to crash.</br>
The reason we opted for a half batch and not a full batch by doing it twice is to train the discriminator more than the model, as suggested in the slides.
"""

def train(num_iter):
    d_losses = []
    g_losses = []
    for i in range(num_iter):
        # Train discriminator
        # Generate fake images
        fake_images = []
        g_loss = 0
        for k in range(batch_size // 16):
          noise = np.random.uniform(-1, 1, (batch_size // 2, 320, 320, 3))
          fake_images.append(generator.predict(noise))
        fake_images = np.array(fake_images)
        fake_images = fake_images.reshape((batch_size, 320, 320, 3))
        # Get real images
        # Combine real and fake images
        x = np.concatenate((real_images, fake_images))
        # Create labels
        y = np.concatenate((np.random.uniform(0.9, 1,(batch_size, 1)), np.random.uniform(0, 0.1,(batch_size, 1))))
        # Train discriminator
        d_loss = discriminator.train_on_batch(x, y)
        d_losses.append(d_loss)
        # Train generator
        noise = np.random.uniform(-1, 1, (batch_size // 2, 320, 320, 3))
        y = np.ones((batch_size // 2, 1))
        g_loss += gan.train_on_batch(noise, y)
        g_losses.append(g_loss)
        print(f"Epoch: {i}, d loss: {d_loss}, g loss: {g_loss}")
    return d_losses, g_losses

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# d_losses, g_losses = train(300)

"""Likewise, the generation function needed to be changed."""

def generate_and_show():
  images = []
  for i in range(batch_size // 16):
    noise = np.random.uniform(-1, 1, (batch_size // 2, 320, 320, 3))
    generated_image = generator.predict(noise)
    images.append(generated_image)
  images = np.array(images)
  images = images.reshape(batch_size, 320, 320, 3)

  # Convert the image back to the original scale
  images = (images + 1) * 127.5

  # Convert the image to uint8
  images = np.uint8(images)

  # Create a grid of 8x4 subplots
  fig, axes = plt.subplots(8, 4)

  fig.set_size_inches(18, 32)

  # Flatten the axes array so we can iterate through it easily
  axes = axes.ravel()

  # Iterate through the list of images and display them in the subplots
  for i, img in enumerate(images):
      axes[i].imshow(img)

  # Show the plot
  plt.show()

"""### Results

The loss graph shows a noisy process at first, but then a stable one later on.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# plot_losses(d_losses, g_losses)

"""The results are... not good.</br>
They look nothing like a painting, and barely resemble the style at all.</br>
"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# generate_and_show()

"""Therefore, we have decided to train the model for another 150 iterations."""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# d_losses, g_losses = train(150)

"""The loss graph, that is now on a much smaller scale, shows that the loss of both constantly went up and down."""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# plot_losses(d_losses, g_losses)

"""And the results are possibly even worse now, and frankly, also mildly horrifying."""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# generate_and_show()

"""Overall, this architecture is a failure.

## Improvement attempt 3 - dropout layers

### Architecture
Because the previous model's architecture failed, we have elected to return to the generator architecture of our first attempt.</br>
We did, however, keep some batch normalization layers.</br>
</br>
In addition, we added dropout layers to the discriminator and generator to try and prevent overfitting, as the first attempt's model seemed to be overfitted, as it was generating mostly just 1 type of painting.</br></br>

Also, at this point, we returned to the previous training process, before adjusting it to the memory requirements of the previous attempt.
"""

def build_generator():
    model = Sequential()
    model.add(Input(shape=generator_output_shape))
    model.add(Flatten())
    model.add(Dense(256))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.1))
    model.add(Dense(516))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.1))
    model.add(BatchNormalization(momentum=0.9))
    model.add(Dense(256))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.1))
    model.add(Dense(np.prod(generator_output_shape)))
    model.add(layers.LeakyReLU())
    model.add(Reshape(generator_output_shape))
    model.add(layers.Conv2D(32, (5, 5), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.1))
    model.add(BatchNormalization(momentum=0.9))
    model.add(layers.Conv2D(32, (5, 5), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(3, (5, 5), padding='same', activation='tanh'))
    return model

# Define the discriminator network
def build_discriminator():
    model = Sequential()
    model.add(layers.Conv2D(32, (5, 5), padding='same', input_shape=generator_output_shape))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.1))
    model.add(layers.Conv2D(48, (5, 5), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.1))
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(64, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(128, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.1))
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(256, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.1))
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(384, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.Flatten())
    model.add(layers.Dense(1, activation='sigmoid'))
    return model

gan = Sequential()
discriminator = build_discriminator()
discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate = 0.0008, beta_1=0.5))
discriminator.trainable = False
discriminator.summary()
generator = build_generator()
generator.summary()
gan.add(generator)
gan.add(discriminator)
gan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate = 0.0004, beta_1=0.5))

"""### Training and results"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# d_losses, g_losses = train(300)

"""The loss graph now looks much more similar to that of the first improvement attempt - which makes sense, their architectures are very similar, short of the batch normalizations and dropout layers."""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# plot_losses(d_losses, g_losses)

"""The results however are also similar in a way - the model is fixated on a single type of painting.</br>
It also seems to have not picked up on the style quite as well as the previous time.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# generate_and_show()

"""## Improvement attempt 4 - lower learning rate
This time, we have set the learning rate of both models to 0.0001, to try and prevent overfitting even more.
"""

gan = Sequential()
discriminator = build_discriminator()
discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate = 0.0001, beta_1=0.5))
discriminator.trainable = False
discriminator.summary()
generator = build_generator()
generator.summary()
gan.add(generator)
gan.add(discriminator)
gan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate = 0.0001, beta_1=0.5))

"""### Training and results"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# d_losses, g_losses = train(300)

"""The loss graph seems to constantly fluctuate for both models, which suggests that both are learning."""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# plot_losses(d_losses, g_losses)

"""And the results, while not paintings just yet, do seem to not fixate on a single painting completely."""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# generate_and_show()

"""We gave it 200 more epochs to train."""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# d_losses, g_losses = train(200)

"""The loss graphs still seem to fluctuate a lot, which suggests the model is still learning and adapting."""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# plot_losses(d_losses, g_losses)

"""And the images look more like paintings, while it still seems like there is more than 1 painting being represented."""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# generate_and_show()

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# d_losses, g_losses = train(300)

"""Once more, there is a lot of fluctuation for both models."""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# plot_losses(d_losses, g_losses)

"""However, the results do not look good, as they barely appear to be Monet paintings, if at all."""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# generate_and_show()

"""## Improvement attempt 5 - without the dropout layers

While dropout layers are generally good for preventing overfitting, from what it seems like in the previous model, they may have caused too much information loss, as can be seen by some of the generated images being practically just a single color.</br>
As such, we decided to try the same model - the modified first improvement's architecture with batch normalizations, but without dropout layers, and with the same learning rate of the previous attempt.

### Architecture
"""

def build_generator():
    model = Sequential()
    model.add(Input(shape=generator_output_shape))
    model.add(Flatten())
    model.add(Dense(256))
    model.add(layers.LeakyReLU())
    model.add(Dense(516))
    model.add(layers.LeakyReLU())
    model.add(BatchNormalization(momentum=0.9))
    model.add(Dense(256))
    model.add(layers.LeakyReLU())
    model.add(Dense(np.prod(generator_output_shape)))
    model.add(layers.LeakyReLU())
    model.add(Reshape(generator_output_shape))
    model.add(layers.Conv2D(32, (5, 5), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(BatchNormalization(momentum=0.9))
    model.add(layers.Conv2D(32, (5, 5), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(3, (5, 5), padding='same', activation='tanh'))
    return model

# Define the discriminator network
def build_discriminator():
    model = Sequential()
    model.add(layers.Conv2D(32, (5, 5), padding='same', input_shape=generator_output_shape))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(48, (5, 5), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(64, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(128, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(256, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(384, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.Flatten())
    model.add(layers.Dense(1, activation='sigmoid'))
    return model

gan = Sequential()
discriminator = build_discriminator()
discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate = 0.0001, beta_1=0.5))
discriminator.trainable = False
discriminator.summary()
generator = build_generator()
generator.summary()
gan.add(generator)
gan.add(discriminator)
gan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate = 0.0001, beta_1=0.5))

"""### Training and results"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# d_losses, g_losses = train(300)

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# plot_losses(d_losses, g_losses)

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# generate_and_show()

"""## Improvement attempt 6 - patch GAN and emulating U-net

As could be seen, the previous attempts at improving the model did not work out well.</br>
As such, we have decided to try 2 things:</br>
1. To use a patch GAN, in hopes it would improve the model and possibly help with fixing the fixation issue of it.
2. Emulating a well known architecture that is known to work, specifically U-net.</br>
</br>
We are basing this on the tutorials seen <a href="https://machinelearningmastery.com/how-to-implement-pix2pix-gan-models-from-scratch-with-keras/ target="_blank">here</a> and <a href="https://sahiltinky94.medium.com/understanding-patchgan-9f3c8380c207" target="_blank">here</a>.

### Architecture

We will be using a Patch GAN with a receptive field of 94 x 94
"""

def receptive_field(output_size, kernel_size, stride_size):
    return (output_size - 1) * stride_size + kernel_size

rf = receptive_field(1, 4, 1)
print(rf)
rf = receptive_field(rf, 4, 2)
print(rf)
rf = receptive_field(rf, 4, 2)
print(rf)
rf = receptive_field(rf, 4, 2)
print(rf)
rf = receptive_field(rf, 4, 2)
print(rf)

"""The generator will first downsample the image, then upsample it again, working similarly to U-net."""

from keras.initializers import RandomNormal

def build_generator():
    model = Sequential()
    model.add(Input(shape=generator_output_shape))
    init = RandomNormal(stddev=0.02)

    model.add(layers.Conv2D(64, (4,4), strides=2, padding='same', kernel_initializer=init))
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Conv2D(128, (4,4), strides=2, padding='same', kernel_initializer=init))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Conv2D(256, (4,4), strides=2, padding='same', kernel_initializer=init))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Conv2D(512, (4,4), strides=2, padding='same', kernel_initializer=init))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Conv2D(512, (4,4), strides=2, padding='same', kernel_initializer=init))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Conv2D(512, (4,4), strides=2, padding='same', kernel_initializer=init))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU(0.2))

    model.add(layers.Conv2DTranspose(512, (4,4), strides=2, padding='same', kernel_initializer=init))
    model.add(layers.BatchNormalization())
    model.add(layers.Dropout(0.3))
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Conv2DTranspose(512, (4,4), strides=2, padding='same', kernel_initializer=init))
    model.add(layers.BatchNormalization())
    model.add(layers.Dropout(0.3))
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Conv2DTranspose(512, (4,4), strides=2, padding='same', kernel_initializer=init))
    model.add(layers.BatchNormalization())
    model.add(layers.Dropout(0.3))
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Conv2DTranspose(512, (4,4), strides=2, padding='same', kernel_initializer=init))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Conv2DTranspose(256, (4,4), strides=2, padding='same', kernel_initializer=init))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Conv2DTranspose(128, (4,4), strides=2, padding='same', kernel_initializer=init))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU(0.2))

    model.add(Conv2DTranspose(3, (4,4), padding='same', kernel_initializer=init, activation='tanh'))

    return model

# Define the discriminator network
def build_discriminator():
    model = Sequential()
    init = RandomNormal(stddev=0.02)
    model.add(layers.Input(generator_output_shape))
    model.add(layers.Conv2D(64, (4, 4), strides=2, padding='same', kernel_initializer=init))
    model.add(layers.LeakyReLU(0.1))
    model.add(layers.Conv2D(128, (4, 4), strides=2, padding='same', kernel_initializer=init))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU(0.1))
    model.add(layers.Conv2D(256, (4, 4), strides=2, padding='same', kernel_initializer=init))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU(0.1))
    model.add(layers.Conv2D(512, (4, 4), strides=2, padding='same', kernel_initializer=init))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU(0.1))
    model.add(layers.Conv2D(1, (4, 4), strides=1, padding='same', kernel_initializer=init, activation='sigmoid'))
    return model

"""### Hyper-parameters

We have also decided to use the learning rate and optimizer from the tutorial, as overall, the Adam optimizer should be the best optimizer to use, and a learning rate of 0.0002 is very reasonable to use.
"""

gan = Sequential()
discriminator = build_discriminator()
discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate = 0.0002, beta_1=0.5))
discriminator.trainable = False
discriminator.summary()
generator = build_generator()
generator.summary()
gan.add(generator)
gan.add(discriminator)
gan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate = 0.0002, beta_1=0.5))

"""### Training and results

We needed to make a slight change to the training function, as the discriminator now outputs an array of values, instead of just 1.
"""

import tensorflow as tf
tf.config.run_functions_eagerly(True)

def train(num_iter):
    d_losses = []
    g_losses = []
    for i in range(num_iter):
        # Train discriminator
        # Generate fake images
        noise = np.random.uniform(-1, 1, (batch_size, 320, 320, 3))
        fake_images = generator.predict(noise)
        # Get real images
        # Combine real and fake images
        x = np.concatenate((real_images, fake_images))
        # Create labels
        y = np.concatenate((np.random.uniform(0.9, 1,(batch_size,20, 20, 1)), np.random.uniform(0, 0.1,(batch_size,20, 20, 1))))
        # Train discriminator
        d_loss = discriminator.train_on_batch(x, y)
        d_losses.append(d_loss)
        # Train generator
        noise = np.random.uniform(-1, 1, (batch_size, 320, 320, 3))
        y = np.ones((batch_size, 20, 20, 1))
        g_loss = gan.train_on_batch(noise, y)
        g_losses.append(g_loss)
        print('Iteration: {}, Discriminator loss: {}, Generator loss: {}'.format(i, d_loss, g_loss))
    return d_losses, g_losses

"""Like the previous models, we gave it 300 epochs for its training."""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# d_losses, g_losses = train(300)

"""While the loss graph shows some fluctuations and variance, its overall loss values seem to be lower than previous attempts, which made us optimistic."""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# plot_losses(d_losses, g_losses)

"""However, that optimism was crushed by the results it generated.</br>
The results look nothing like the Monet style, and are also all practically the same image, with minor differences in color here and there.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# generate_and_show()

"""## Summary and conclusions

One thing that could be noticed with every model is that almost all of them <b>DID</b> manage to pick up on the painting's style quite well.</br>
However, the main issue with them is that they tended to get fixated on a single painting, and generating only that, an issue that is very hard to fix due to how limited our data is - something which makes the models much more prone to this.</br>
We have attempted to negate this behavior via dropout layers, batch normalizations, and via a lower learning rate, both of which did not seem to help much.</br>
As a result, the overall best model is the first improvement attempt's model, with its hyper-parameters.</br>
As for what we got to learn from this, is that sometimes more complex architecture may not serve our purposes - as we have seen when trying to make the generator more complex, and that memory is a major concern apparently when choosing architecture, as not having enough could lead the model to crash, which was another limiting factor in designing architecture that could have maybe fixed the problem of overfitting (although not likely).

### Re-training the best model
As we forgot to save the model, here is the model being trained again and saved this time - this trained version of it is the one in the test environment as well.
"""

def build_generator():
    model = Sequential()
    model.add(Input(shape=generator_output_shape))
    model.add(Flatten())
    model.add(Dense(256))
    model.add(layers.LeakyReLU())
    model.add(Dense(516))
    model.add(layers.LeakyReLU())
    model.add(Dense(256))
    model.add(layers.LeakyReLU())
    model.add(Dense(np.prod(generator_output_shape)))
    model.add(layers.LeakyReLU())
    model.add(Reshape(generator_output_shape))
    model.add(layers.Conv2D(32, (5, 5), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(32, (5, 5), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(3, (5, 5), padding='same', activation='tanh'))
    return model

# Define the discriminator network
def build_discriminator():
    model = Sequential()
    model.add(layers.Conv2D(32, (5, 5), padding='same', input_shape=generator_output_shape))
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2D(48, (5, 5), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(64, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(128, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(256, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(384, (3, 3)))
    model.add(layers.LeakyReLU())
    model.add(layers.Flatten())
    model.add(layers.Dense(1, activation='sigmoid'))
    return model

gan = Sequential()
discriminator = build_discriminator()
discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.RMSprop(lr=0.0008, clipvalue=1.0, decay=1e-8))
discriminator.trainable = False
discriminator.summary()
generator = build_generator()
generator.summary()
gan.add(generator)
gan.add(discriminator)
gan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8))

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# d_losses, g_losses = train(300)

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# generate_and_show()

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# generator.save("best_simple_GAN_generator")

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# from google.colab import drive
# drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%script echo skipping
# !cp -r best_simple_GAN_generator "/content/drive/MyDrive/Deep Learning/Final"